#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­æ–‡å¤šè½®å¯¹è¯æƒ…ç»ªè¯†åˆ«è®­ç»ƒè„šæœ¬ - è‡ªåŠ¨æ£€æµ‹ç‰¹å¾ç»´åº¦ç‰ˆæœ¬
æ”¯æŒä¸åŒçš„ç‰¹å¾æå–ç­–ç•¥ï¼ˆè‡ªåŠ¨é€‚é…ç‰¹å¾ç»´åº¦ï¼‰
"""

import numpy as np
import argparse
import time
import pickle
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from chinese_dataloader import ChineseDataset
from model import MaskedNLLLoss, LSTMModel, GRUModel, DialogRNNModel, DialogueGCNModel
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report
import os
import json

seed = 100

def seed_everything(seed=seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def detect_feature_dimension(pkl_path):
    """
    è‡ªåŠ¨æ£€æµ‹ç‰¹å¾ç»´åº¦
    
    Returns:
        feature_dim: ç‰¹å¾ç»´åº¦
        pooling_type: æ£€æµ‹åˆ°çš„æ± åŒ–ç±»å‹
    """
    print("æ­£åœ¨è‡ªåŠ¨æ£€æµ‹ç‰¹å¾ç»´åº¦...")
    
    try:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
            
        # dataæ ¼å¼: (video_ids, video_speakers, video_labels, video_text, trainVids, testVids)
        video_ids, video_speakers, video_labels, video_text, trainVids, testVids = data
        
        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾
        first_vid = video_ids[0]
        first_features = video_text[first_vid]
        
        if len(first_features) > 0:
            feature_dim = len(first_features[0])
        else:
            raise ValueError("æ— æ³•æ£€æµ‹ç‰¹å¾ç»´åº¦ï¼šæ•°æ®ä¸ºç©º")
        
        # æ ¹æ®ç»´åº¦åˆ¤æ–­æ± åŒ–ç±»å‹
        if feature_dim == 768:
            pooling_type = "å•ä¸€æ± åŒ– (cls/mean/max/attention)"
        elif feature_dim == 2304:
            pooling_type = "å¤šæ± åŒ–ç»„åˆ (multi)"
        else:
            pooling_type = f"è‡ªå®šä¹‰ç»´åº¦"
        
        print(f"âœ“ æ£€æµ‹å®Œæˆ:")
        print(f"  ç‰¹å¾ç»´åº¦: {feature_dim}")
        print(f"  æ± åŒ–ç±»å‹: {pooling_type}")
        print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(trainVids)}")
        print(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(testVids)}")
        
        return feature_dim, pooling_type
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾ç»´åº¦æ£€æµ‹å¤±è´¥: {e}")
        print("ä½¿ç”¨é»˜è®¤ç»´åº¦768")
        return 768, "é»˜è®¤"


def get_chinese_loaders(pkl_path, batch_size=32, num_workers=0, pin_memory=False):
    """è·å–ä¸­æ–‡æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨"""
    trainset = ChineseDataset(pkl_path, 'train')
    validset = ChineseDataset(pkl_path, 'valid')
    testset = ChineseDataset(pkl_path, 'test')

    train_loader = DataLoader(trainset,
                              batch_size=batch_size,
                              collate_fn=trainset.collate_fn,
                              num_workers=num_workers,
                              pin_memory=pin_memory,
                              shuffle=True)

    valid_loader = DataLoader(validset,
                              batch_size=batch_size,
                              collate_fn=validset.collate_fn,
                              num_workers=num_workers,
                              pin_memory=pin_memory)

    test_loader = DataLoader(testset,
                             batch_size=batch_size,
                             collate_fn=testset.collate_fn,
                             num_workers=num_workers,
                             pin_memory=pin_memory)

    return train_loader, valid_loader, test_loader


def train_or_eval_graph_model(model, loss_function, dataloader, epoch, cuda, optimizer=None, train=False):
    """è®­ç»ƒæˆ–è¯„ä¼°å›¾æ¨¡å‹"""
    losses, preds, labels = [], [], []
    
    ei, et, en, el = torch.empty(0).type(torch.LongTensor), torch.empty(0).type(torch.LongTensor), torch.empty(0), []

    if cuda:
        ei, et, en = ei.cuda(), et.cuda(), en.cuda()

    assert not train or optimizer is not None
    if train:
        model.train()
    else:
        model.eval()

    seed_everything()
    for data in dataloader:
        if train:
            optimizer.zero_grad()
        
        textf, visuf, acouf, qmask, umask, label = [d.cuda() for d in data[:-1]] if cuda else data[:-1]

        lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]

        log_prob, e_i, e_n, e_t, e_l = model(textf, qmask, umask, lengths)
        label = torch.cat([label[j][:lengths[j]] for j in range(len(label))])
        loss = loss_function(log_prob, label)

        ei = torch.cat([ei, e_i], dim=1)
        et = torch.cat([et, e_t])
        en = torch.cat([en, e_n])
        el += e_l

        preds.append(torch.argmax(log_prob, 1).cpu().numpy())
        labels.append(label.cpu().numpy())
        losses.append(loss.item())

        if train:
            loss.backward()
            optimizer.step()

    if preds != []:
        preds = np.concatenate(preds)
        labels = np.concatenate(labels)
    else:
        return float('nan'), float('nan'), [], [], float('nan')

    avg_loss = round(np.sum(losses) / len(losses), 4)
    avg_accuracy = round(accuracy_score(labels, preds) * 100, 2)
    avg_fscore = round(f1_score(labels, preds, average='weighted') * 100, 2)

    return avg_loss, avg_accuracy, labels, preds, avg_fscore


def save_results(results, output_dir):
    """ä¿å­˜è®­ç»ƒç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    
    with open(os.path.join(output_dir, 'training_history.json'), 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description='ä¸­æ–‡å¤šè½®å¯¹è¯æƒ…ç»ªè¯†åˆ«è®­ç»ƒï¼ˆè‡ªåŠ¨ç‰¹å¾ç»´åº¦æ£€æµ‹ï¼‰')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument('--data_path', required=True, help='pklæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', default='./saved/chinese_enhanced/', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    
    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument('--no-cuda', action='store_true', default=False, help='ä¸ä½¿ç”¨GPU')
    parser.add_argument('--base-model', default='LSTM', help='åŸºç¡€å¾ªç¯æ¨¡å‹ (DialogRNN/LSTM/GRU)')
    parser.add_argument('--graph-model', action='store_true', default=False, help='æ˜¯å¦ä½¿ç”¨å›¾æ¨¡å‹')
    parser.add_argument('--nodal-attention', action='store_true', default=False, help='æ˜¯å¦ä½¿ç”¨èŠ‚ç‚¹æ³¨æ„åŠ›')
    parser.add_argument('--windowp', type=int, default=10, help='è¿‡å»çª—å£å¤§å°')
    parser.add_argument('--windowf', type=int, default=10, help='æœªæ¥çª—å£å¤§å°')
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--lr', type=float, default=0.0001, help='å­¦ä¹ ç‡')
    parser.add_argument('--l2', type=float, default=0.00001, help='L2æ­£åˆ™åŒ–æƒé‡')
    parser.add_argument('--rec-dropout', type=float, default=0.1, help='å¾ªç¯å±‚dropoutç‡')
    parser.add_argument('--dropout', type=float, default=0.5, help='dropoutç‡')
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--epochs', type=int, default=60, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--class-weight', action='store_true', default=False, help='ä½¿ç”¨ç±»åˆ«æƒé‡')
    parser.add_argument('--active-listener', action='store_true', default=False, help='ä¸»åŠ¨ç›‘å¬å™¨')
    parser.add_argument('--attention', default='general', help='æ³¨æ„åŠ›ç±»å‹')
    
    # ç‰¹å¾ç»´åº¦ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šè‡ªåŠ¨æ£€æµ‹ï¼‰
    parser.add_argument('--feature-dim', type=int, default=None, help='ç‰¹å¾ç»´åº¦ï¼ˆä¸æä¾›åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰')
    
    args = parser.parse_args()
    print("="*60)
    print("è®­ç»ƒå‚æ•°:", args)
    print("="*60)

    # æ£€æŸ¥CUDA
    args.cuda = torch.cuda.is_available() and not args.no_cuda
    if args.cuda:
        print('âœ“ ä½¿ç”¨GPUè®­ç»ƒ')
        print(f'  GPU: {torch.cuda.get_device_name(0)}')
    else:
        print('âš  ä½¿ç”¨CPUè®­ç»ƒ')

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(args.data_path):
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_path}")
        return

    # è‡ªåŠ¨æ£€æµ‹ç‰¹å¾ç»´åº¦
    if args.feature_dim is None:
        D_m, pooling_type = detect_feature_dimension(args.data_path)
    else:
        D_m = args.feature_dim
        pooling_type = "æ‰‹åŠ¨æŒ‡å®š"
        print(f"ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„ç‰¹å¾ç»´åº¦: {D_m}")
    
    # è®¾ç½®éšæœºç§å­
    seed_everything()

    # æ¨¡å‹å‚æ•°
    # æ³¨æ„ï¼šéœ€è¦æ ¹æ®å®é™…æ•°æ®ä¸­çš„æƒ…ç»ªç±»åˆ«æ•°é‡è®¾ç½®
    # ChineseDatasetå®šä¹‰äº†6ç§æƒ…ç»ª: neutral, happy, sad, angry, fear, surprise
    n_classes = 4
    cuda = args.cuda
    n_epochs = args.epochs
    batch_size = args.batch_size

    # å…¶ä»–ç»´åº¦å‚æ•°
    D_g = 150
    D_p = 150
    D_e = 100
    D_h = 100
    D_a = 100
    graph_h = 100
    
    # å¦‚æœç‰¹å¾ç»´åº¦å¾ˆå¤§ï¼Œè°ƒæ•´æ¨¡å‹å†…éƒ¨ç»´åº¦
    if D_m > 1000:
        print(f"æ£€æµ‹åˆ°å¤§ç»´åº¦ç‰¹å¾({D_m})ï¼Œè°ƒæ•´æ¨¡å‹å†…éƒ¨ç»´åº¦...")
        D_g = 200
        D_p = 200
        D_e = 150
        D_h = 150
        graph_h = 150

    print(f"\næ¨¡å‹é…ç½®:")
    print(f"  ç‰¹å¾ç»´åº¦ (D_m): {D_m}")
    print(f"  å…¨å±€ç»´åº¦ (D_g): {D_g}")
    print(f"  è¯´è¯è€…ç»´åº¦ (D_p): {D_p}")
    print(f"  æƒ…ç»ªç»´åº¦ (D_e): {D_e}")
    print(f"  éšè—ç»´åº¦ (D_h): {D_h}")
    print(f"  å›¾ç»´åº¦ (graph_h): {graph_h}")

    # åˆ›å»ºæ¨¡å‹
    if args.graph_model:
        model = DialogueGCNModel(args.base_model,
                                 D_m, D_g, D_p, D_e, D_h, D_a, graph_h,
                                 n_speakers=4,  # ChineseDatasetå®šä¹‰äº†4ä¸ªè¯´è¯è€…: A, B, C, D
                                 max_seq_len=300,  # å¢åŠ åˆ°300ä»¥æ”¯æŒæ›´é•¿çš„å¯¹è¯åºåˆ—
                                 window_past=args.windowp,
                                 window_future=args.windowf,
                                 n_classes=n_classes,
                                 listener_state=args.active_listener,
                                 context_attention=args.attention,
                                 dropout=args.dropout,
                                 nodal_attention=args.nodal_attention,
                                 no_cuda=args.no_cuda)
        print(f'\nâœ“ ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ŒåŸºç¡€æ¨¡å‹: {args.base_model}')
        name = 'Graph'
    else:
        if args.base_model == 'DialogRNN':
            model = DialogRNNModel(D_m, D_g, D_p, D_e, D_h, D_a, 
                                   n_classes=n_classes,
                                   listener_state=args.active_listener,
                                   context_attention=args.attention,
                                   dropout_rec=args.rec_dropout,
                                   dropout=args.dropout)
        elif args.base_model == 'GRU':
            model = GRUModel(D_m, D_e, D_h, 
                              n_classes=n_classes, 
                              dropout=args.dropout)
        elif args.base_model == 'LSTM':
            model = LSTMModel(D_m, D_e, D_h, 
                              n_classes=n_classes, 
                              dropout=args.dropout)
        else:
            print('åŸºç¡€æ¨¡å‹å¿…é¡»æ˜¯ DialogRNN/LSTM/GRU ä¹‹ä¸€')
            raise NotImplementedError
        
        print(f'\nâœ“ ä½¿ç”¨åŸºç¡€æ¨¡å‹: {args.base_model}')
        name = 'Base'

    if cuda:
        model.cuda()
    
    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\næ¨¡å‹å‚æ•°:")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # è·å–æ•°æ®åŠ è½½å™¨ï¼ˆå¿…é¡»å…ˆåˆ›å»ºï¼Œæ‰èƒ½è®¡ç®—class_weightï¼‰
    try:
        train_loader, valid_loader, test_loader = get_chinese_loaders(
            args.data_path, 
            batch_size=batch_size, 
            num_workers=0
        )
        print(f"\nâœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # è®¾ç½®æŸå¤±å‡½æ•°
    if args.class_weight:
        # æ™ºèƒ½è®¡ç®—ç±»åˆ«æƒé‡
        print("\nè®¡ç®—ç±»åˆ«æƒé‡...")
        
        # æ”¶é›†è®­ç»ƒé›†æ ‡ç­¾ï¼ˆåªç»Ÿè®¡æœ‰æ•ˆä½ç½®ï¼Œä¸åŒ…å«paddingï¼‰
        all_train_labels = []
        for data in train_loader:
            # æ•°æ®æ ¼å¼: text_features, visual_features, audio_features, speaker_features, mask, labels, vid
            _, _, _, _, mask, labels, _ = data
            # ä½¿ç”¨maskè¿‡æ»¤æ‰paddingä½ç½®
            mask_np = mask.cpu().numpy()
            labels_np = labels.cpu().numpy()
            for i in range(labels_np.shape[0]):  # batchä¸­çš„æ¯ä¸ªå¯¹è¯
                valid_length = int(mask_np[i].sum())  # æœ‰æ•ˆé•¿åº¦
                all_train_labels.extend(labels_np[i, :valid_length])  # åªå–æœ‰æ•ˆæ ‡ç­¾
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        from collections import Counter
        class_counts = Counter(all_train_labels)
        n_samples = len(all_train_labels)
        num_classes = n_classes  # é¿å…å˜é‡åå†²çª
        
        print("è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
        emotion_names = ['NEUTRAL', 'HAPPY', 'SAD', 'ANGRY']
        for cls in sorted(class_counts.keys()):
            count = class_counts[cls]
            ratio = count / n_samples * 100
            name = emotion_names[int(cls)] if int(cls) < len(emotion_names) else f"Class_{cls}"
            print(f"  {name:8s}: {count:4d} æ ·æœ¬ ({ratio:5.2f}%)")
        
        # ä½¿ç”¨æ¸©å’Œçš„æƒé‡ç­–ç•¥ï¼šsqrt + alpha=0.5
        # é¿å…è¿‡åº¦æƒ©ç½šå¤šæ•°ç±»æˆ–è¿‡åº¦æå‡å°‘æ•°ç±»
        loss_weights_np = np.zeros(num_classes)
        for cls in range(num_classes):
            if cls in class_counts:
                # å¹³æ–¹æ ¹ç­–ç•¥ï¼šæ›´æ¸©å’Œçš„æƒé‡
                loss_weights_np[cls] = np.sqrt(n_samples / class_counts[cls])
            else:
                loss_weights_np[cls] = 1.0
        
        # å½’ä¸€åŒ–åˆ°æœ€å°æƒé‡ä¸º1
        loss_weights_np = loss_weights_np / loss_weights_np.min()
        
        # åº”ç”¨alpha=0.35ï¼ˆå¹³è¡¡å€¼ï¼‰
        # alphaè¶Šå°ï¼Œæƒé‡å·®å¼‚è¶Šå°ï¼Œå¯¹å°‘æ•°ç±»çš„åå‘è¶Šå¼±
        # 0.35æ˜¯åœ¨0.2ï¼ˆå¤ªä¿å®ˆï¼‰å’Œ0.5ï¼ˆå¤ªæ¿€è¿›ï¼‰ä¹‹é—´çš„å¹³è¡¡ç‚¹
        loss_weights_np = 1.0 + (loss_weights_np - 1.0) * 0.35
        
        # è®¾ç½®æƒé‡ä¸Šé™ï¼Œé˜²æ­¢è¿‡åº¦åå‘å°‘æ•°ç±»
        max_weight = 1.6  # ä»»ä½•ç±»åˆ«çš„æƒé‡æœ€å¤šæ˜¯æœ€å°æƒé‡çš„1.6å€
        loss_weights_np = np.clip(loss_weights_np, 1.0, max_weight)
        
        loss_weights = torch.FloatTensor(loss_weights_np)
        
        print(f"è®¡ç®—å¾—åˆ°çš„ç±»åˆ«æƒé‡: {loss_weights.numpy()}")
        print(f"æƒé‡æ¯”ä¾‹ (æœ€å¤§/æœ€å°): {loss_weights.max()/loss_weights.min():.2f}x")
        
        if args.graph_model:
            loss_function = nn.NLLLoss(loss_weights.cuda() if cuda else loss_weights)
        else:
            loss_function = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)
    else:
        if args.graph_model:
            loss_function = nn.NLLLoss()
        else:
            loss_function = MaskedNLLLoss()

    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)

    # è®­ç»ƒå¾ªç¯
    best_fscore, best_loss, best_label, best_pred = None, None, None, None
    all_fscore, all_acc, all_loss = [], [], []
    training_history = {
        'train_loss': [], 'train_acc': [], 'train_fscore': [],
        'valid_loss': [], 'valid_acc': [], 'valid_fscore': [],
        'test_loss': [], 'test_acc': [], 'test_fscore': [],
        'config': {
            'feature_dim': D_m,
            'pooling_type': pooling_type,
            'base_model': args.base_model,
            'graph_model': args.graph_model
        }
    }

    print("\n" + "="*60)
    print("å¼€å§‹è®­ç»ƒ...")
    print("="*60)
    
    for e in range(n_epochs):
        start_time = time.time()

        if args.graph_model:
            train_loss, train_acc, _, _, train_fscore = train_or_eval_graph_model(
                model, loss_function, train_loader, e, cuda, optimizer, True)
            valid_loss, valid_acc, _, _, valid_fscore = train_or_eval_graph_model(
                model, loss_function, valid_loader, e, cuda)
            test_loss, test_acc, test_label, test_pred, test_fscore = train_or_eval_graph_model(
                model, loss_function, test_loader, e, cuda)
        else:
            # ä½¿ç”¨éå›¾æ¨¡å‹çš„è®­ç»ƒå‡½æ•°
            print("è¯·ä½¿ç”¨--graph-modelå‚æ•°")
            return

        # è®°å½•è®­ç»ƒå†å²
        training_history['train_loss'].append(float(train_loss))
        training_history['train_acc'].append(float(train_acc))
        training_history['train_fscore'].append(float(train_fscore))
        training_history['valid_loss'].append(float(valid_loss))
        training_history['valid_acc'].append(float(valid_acc))
        training_history['valid_fscore'].append(float(valid_fscore))
        training_history['test_loss'].append(float(test_loss))
        training_history['test_acc'].append(float(test_acc))
        training_history['test_fscore'].append(float(test_fscore))

        all_fscore.append(test_fscore)
        all_acc.append(test_acc)
        all_loss.append(test_loss)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output_dir, exist_ok=True)
        
        # ä¿å­˜æ¯ä¸ªepochçš„æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(args.output_dir, f'checkpoint_epoch_{e+1:02d}_{name}_{args.base_model}.pkl')
        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': e,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'train_fscore': train_fscore,
            'valid_loss': valid_loss,
            'valid_acc': valid_acc,
            'valid_fscore': valid_fscore,
            'test_loss': test_loss,
            'test_acc': test_acc,
            'test_fscore': test_fscore,
            'feature_dim': D_m,
            'pooling_type': pooling_type,
            'args': args
        }, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if best_fscore is None or test_fscore > best_fscore:
            best_fscore = test_fscore
            best_loss = test_loss
            best_label = test_label
            best_pred = test_pred
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            best_model_path = os.path.join(args.output_dir, f'best_model_{name}_{args.base_model}.pkl')
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': e,
                'test_fscore': test_fscore,
                'test_acc': test_acc,
                'feature_dim': D_m,
                'pooling_type': pooling_type,
                'args': args
            }, best_model_path)
            
            print(f'  ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {os.path.basename(best_model_path)}')

        # æ‰“å°è¿›åº¦
        print(f'Epoch {e+1:02d}/{n_epochs}:')
        print(f'  Train - Loss: {train_loss:.4f}, Acc: {train_acc:5.2f}%, F1: {train_fscore:5.2f}%')
        print(f'  Valid - Loss: {valid_loss:.4f}, Acc: {valid_acc:5.2f}%, F1: {valid_fscore:5.2f}%')
        print(f'  Test  - Loss: {test_loss:.4f}, Acc: {test_acc:5.2f}%, F1: {test_fscore:5.2f}% {"â­ BEST" if test_fscore == best_fscore else ""}')
        print(f'  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {os.path.basename(checkpoint_path)}')
        print(f'  Time: {time.time()-start_time:.2f}s\n')

    # ä¿å­˜è®­ç»ƒç»“æœ
    save_results(training_history, args.output_dir)

    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f'æœ€ä½³æµ‹è¯•F1åˆ†æ•°: {max(all_fscore):.2f}%')
    print(f'æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {max(all_acc):.2f}%')
    print(f'æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}')

    # æ‰“å°åˆ†ç±»æŠ¥å‘Š
    if best_label is not None and best_pred is not None:
        print('\n=== åˆ†ç±»æŠ¥å‘Š ===')
        emotion_names = ['neutral', 'happy', 'sad', 'angry']
        print(classification_report(best_label, best_pred, target_names=emotion_names))


if __name__ == '__main__':
    main()

