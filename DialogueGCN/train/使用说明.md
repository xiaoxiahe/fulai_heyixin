# 中文多轮对话情绪识别训练系统

## 概述

本系统基于DialogueGCN模型，专门用于中文多轮对话的情绪识别任务。系统包含数据预处理、模型训练、预测等完整功能。

## 文件结构

```
DialogueGCN/
├── chinese_dataloader.py          # 中文数据集数据加载器
├── preprocess_chinese_data.py     # 数据预处理脚本
├── train_chinese.py              # 中文训练脚本
├── emotion_mapping.py            # 情绪标签映射工具
├── quick_start.py                # 快速启动脚本
├── example_usage.py              # 使用示例
├── 使用说明.md                   # 本文件
└── README_chinese_training.md    # 详细训练指南
```

## 快速开始

### 1. 环境准备

确保安装了必要的依赖包：

```bash
pip install torch torchvision torchaudio
pip install transformers
pip install scikit-learn
pip install pandas numpy
pip install tqdm
```

### 2. 数据预处理

将CSV格式的中文对话数据转换为pkl格式：

```bash
# 基本用法
python preprocess_chinese_data.py --input ../datapre/full_data_fixed.csv --output chinese_data.pkl

# 使用GPU加速 (推荐)
python preprocess_chinese_data.py --input ../datapre/full_data_fixed.csv --output chinese_data.pkl --batch_size 64

# 使用CPU (较慢但无需GPU)
python preprocess_chinese_data.py --input ../datapre/full_data_fixed.csv --output chinese_data.pkl --batch_size 16
```

### 3. 开始训练

#### 基础模型训练

```bash
# LSTM模型
python train_chinese.py --data_path chinese_data.pkl --base_model LSTM --epochs 60

# GRU模型
python train_chinese.py --data_path chinese_data.pkl --base_model GRU --epochs 60

# DialogRNN模型
python train_chinese.py --data_path chinese_data.pkl --base_model DialogRNN --epochs 60
```

#### 图神经网络训练 (推荐)

```bash
# DialogueGCN + LSTM
python train_chinese.py --data_path chinese_data.pkl --base_model LSTM --graph_model --nodal_attention --epochs 60

# DialogueGCN + GRU
python train_chinese.py --data_path chinese_data.pkl --base_model GRU --graph_model --nodal_attention --epochs 60

# DialogueGCN + DialogRNN
python train_chinese.py --data_path chinese_data.pkl --base_model DialogRNN --graph_model --nodal_attention --epochs 60
```

### 4. 一键启动

使用快速启动脚本一键完成所有步骤：

```bash
python quick_start.py --input ../datapre/full_data_fixed.csv --model LSTM --use_graph --epochs 60
```

## 详细使用说明

### 数据格式要求

#### 输入CSV格式

CSV文件必须包含以下列：
- `TV_ID`: 对话ID
- `Utterance`: 对话文本
- `Speaker`: 说话者
- `Emotion`: 情绪标签

#### 支持的情绪标签

系统支持以下6种标准情绪：
- `neutral` (中性)
- `happy` (快乐)
- `sad` (悲伤)
- `angry` (愤怒)
- `fear` (恐惧)
- `surprise` (惊讶)

原始数据中的其他情绪标签会自动映射到这些标准标签。

### 模型架构

#### 基础模型
- **LSTM**: 双向LSTM + 注意力机制
- **GRU**: 双向GRU + 注意力机制  
- **DialogRNN**: 对话RNN + 注意力机制

#### 图神经网络
- **DialogueGCN**: 基于图卷积网络的对话情绪识别
- 支持节点注意力机制
- 支持多说话者对话

### 参数说明

#### 数据预处理参数

- `--input`: 输入CSV文件路径
- `--output`: 输出pkl文件路径
- `--bert_model`: BERT模型名称 (默认: bert-base-chinese)
- `--batch_size`: BERT特征提取批处理大小 (默认: 32)

#### 训练参数

- `--data_path`: pkl数据文件路径
- `--output_dir`: 模型保存目录 (默认: ./saved/chinese/)
- `--base_model`: 基础模型 (LSTM/GRU/DialogRNN)
- `--graph_model`: 是否使用图神经网络
- `--nodal_attention`: 是否使用节点注意力
- `--use_bert`: 是否使用BERT特征提取
- `--epochs`: 训练轮数 (默认: 60)
- `--batch_size`: 批处理大小 (默认: 32)
- `--lr`: 学习率 (默认: 0.0001)
- `--dropout`: dropout率 (默认: 0.5)
- `--class_weight`: 使用类别权重处理不平衡数据

## 性能优化建议

### GPU训练
- 使用GPU可以显著加速训练
- 建议batch_size设置为32-64
- 使用BERT特征提取时建议batch_size设置为16-32

### CPU训练
- 建议batch_size设置为8-16
- 关闭BERT特征提取以节省内存
- 使用较小的模型参数

### 内存优化
- 如果内存不足，减小batch_size
- 使用梯度累积来模拟更大的batch_size
- 考虑使用混合精度训练

## 输出文件

训练完成后会生成以下文件：
- `best_model_*.pkl`: 最佳模型文件
- `training_history.json`: 训练历史记录
- 训练日志输出

## 常见问题

### Q: 训练时出现内存不足错误
A: 减小batch_size或使用CPU训练

### Q: BERT特征提取很慢
A: 使用GPU加速或减小batch_size

### Q: 模型性能不佳
A: 尝试不同的基础模型或调整超参数

### Q: 数据预处理失败
A: 检查CSV文件格式和列名是否正确

## 示例用法

```bash
# 完整训练流程
# 1. 数据预处理
python preprocess_chinese_data.py --input data.csv --output chinese_data.pkl

# 2. 分析数据
python emotion_mapping.py --input data.csv --analyze --weights

# 3. 训练模型
python train_chinese.py --data_path chinese_data.pkl --base_model LSTM --graph_model --nodal_attention --epochs 60

# 4. 查看结果
ls saved/chinese/
```

## 技术支持

如果遇到问题，请检查：
1. 数据格式是否正确
2. 依赖包是否安装完整
3. 内存和GPU资源是否充足
4. 文件路径是否正确

## 更新日志

- v1.0: 初始版本，支持基础模型和图神经网络训练
- 支持中文情绪标签映射
- 支持BERT特征提取
- 提供完整的训练和预测流程
